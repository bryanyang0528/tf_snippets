{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I can't recreate this [R2RT](https://r2rt.com/implementing-batch-normalization-in-tensorflow.html) blog post results with `tf.contrib.layers.batch_norm`, I decided to make a second notebook which will follow closely with the implementation in the code.\n",
    "\n",
    "Let's see how far I can go!\n",
    "\n",
    "\\* You can find the details about batch normalization in this [paper](https://arxiv.org/pdf/1502.03167v3.pdf) if you don't know what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1215792b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8E9b8zSskD2orq6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12146d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mnist.train.labels[3690])\n",
    "plt.imshow(mnist.train.images[3690].reshape((28, 28)), cmap='gray', interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training=True, decay=0.999, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    batch_norm_wrapper\n",
    "    \n",
    "    Create a batch normalize layers\n",
    "    \n",
    "    params\n",
    "    ======\n",
    "    - inputs <Tensor>: input Tensor\n",
    "    - is_training <bool>: running in training mode (True by default)\n",
    "    - decay <float>: decay rate of the exponentail moving averages\n",
    "    \n",
    "    returns\n",
    "    =======\n",
    "    - outputs: batch normalized output\n",
    "    \"\"\"\n",
    "    N_features = inputs.get_shape()[-1]\n",
    "    \n",
    "    # Trainable variables for batch normalization\n",
    "    gamma = tf.Variable(tf.ones(N_features), \n",
    "                        dtype=tf.float32)\n",
    "    beta = tf.Variable(tf.zeros(N_features),\n",
    "                       dtype=tf.float32)\n",
    "    # non-trainable variables. Make sure it will not be\n",
    "    # updated with optimizer\n",
    "    moving_mean = tf.Variable(tf.zeros(N_features),\n",
    "                              trainable=False,\n",
    "                              dtype=tf.float32)\n",
    "    moving_var = tf.Variable(tf.ones(N_features),\n",
    "                             trainable=False,\n",
    "                             dtype=tf.float32)\n",
    "    if is_training:\n",
    "        train_mean, train_var = tf.nn.moments(inputs, [0])\n",
    "        update_mean_op = tf.assign(moving_mean, \n",
    "                                   decay*moving_mean+(1-decay)*train_mean)\n",
    "        update_var_op = tf.assign(moving_var,\n",
    "                                  decay*moving_var+(1-decay)*train_var)\n",
    "        with tf.control_dependencies([update_mean_op, update_var_op]):\n",
    "            # use tf.control_dependencies to make sure that update_mean_op and\n",
    "            # update_var_op will be executed before the evaluation of outputs\n",
    "            outputs = tf.nn.batch_normalization(inputs, \n",
    "                                                train_mean, \n",
    "                                                train_var, \n",
    "                                                beta, \n",
    "                                                gamma, \n",
    "                                                epsilon)\n",
    "    else:\n",
    "        # Here we will get the moving average mean an variance from restored \n",
    "        # session. That is, it will normalized inputs with the moving average\n",
    "        # learned from training session\n",
    "        outputs = tf.nn.batch_normalization(inputs,\n",
    "                                            moving_mean,\n",
    "                                            moving_var,\n",
    "                                            beta,\n",
    "                                            gamma,\n",
    "                                            epsilon)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_init = np.random.normal(size=(784, 100)).astype(np.float32)\n",
    "w2_init = np.random.normal(size=(100, 100)).astype(np.float32)\n",
    "w3_init = np.random.normal(size=(100, 10)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(graph, is_training=True):\n",
    "    with graph.as_default():\n",
    "        x_ = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        \n",
    "        # layer 1\n",
    "        W1 = tf.Variable(w1_init, dtype=tf.float32)\n",
    "        z1 = tf.matmul(x_, W1)\n",
    "        l1 = tf.nn.sigmoid(batch_norm_wrapper(z1, is_training=is_training))\n",
    "        \n",
    "        # layer 2\n",
    "        W2 = tf.Variable(w2_init, dtype=tf.float32)\n",
    "        z2 = tf.matmul(l1, W2)\n",
    "        l2 = tf.nn.sigmoid(batch_norm_wrapper(z2, is_training=is_training))\n",
    "        \n",
    "        # layer 3\n",
    "        W3 = tf.Variable(w3_init, dtype=tf.float32)\n",
    "        z3 = tf.matmul(l2, W3)\n",
    "        y = tf.nn.softmax(z3)\n",
    "        predict = tf.arg_max(y, 1)\n",
    "        \n",
    "        loss = -tf.reduce_sum(y_*tf.log(y))\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "    return x_, y_, predict, loss, train_op, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y1, y2):\n",
    "    return 100*(y1 == y2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iteration = 10000\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: 33.61806869506836 90.60%\n",
      "Iteration 2000: 30.719919204711914 92.12%\n",
      "Iteration 3000: 15.241762161254883 93.32%\n",
      "Iteration 4000: 11.810280799865723 93.99%\n",
      "Iteration 5000: 17.605941772460938 94.54%\n",
      "Iteration 6000: 9.061473846435547 94.89%\n",
      "Iteration 7000: 7.425298690795898 95.05%\n",
      "Iteration 8000: 7.866730690002441 95.47%\n",
      "Iteration 9000: 11.218859672546387 95.71%\n",
      "Iteration 10000: 6.200161933898926 95.84%\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "x_, y_, predict, loss, train_op, saver = build_graph(train_graph, True)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(n_iteration):\n",
    "        train_images, train_labels = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {x_:train_images,\n",
    "                     y_:train_labels}\n",
    "        _, l = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        if (step+1) % 1000 == 0:\n",
    "            feed_dict[x_] = mnist.test.images\n",
    "            feed_dict[y_] = mnist.test.labels\n",
    "            pred = sess.run(predict, feed_dict=feed_dict)\n",
    "            acc = compute_accuracy(pred, np.argmax(mnist.test.labels, 1))\n",
    "            print(\"Iteration {}: {} {:.2f}%\".format(step+1, l, acc))\n",
    "    saver.save(sess, \"models/batch_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/batch_norm\n",
      "Accuracy in reference phase: 95.82%\n"
     ]
    }
   ],
   "source": [
    "predict_graph = tf.Graph()\n",
    "x_, y_, predict, _, _, saver = build_graph(predict_graph, False)\n",
    "\n",
    "with tf.Session(graph=predict_graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, \"models/batch_norm\")\n",
    "    \n",
    "    feed_dict = {x_: mnist.test.images,\n",
    "                 y_: mnist.test.labels}\n",
    "    pred = sess.run(predict, feed_dict=feed_dict)\n",
    "    acc = compute_accuracy(pred, np.argmax(mnist.test.labels, 1))\n",
    "    print(\"Accuracy in reference phase: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow....It works. I'll dig into the source codes for both `tf.nn.batch_normalize` and `tf.contrib.layers.batch_norm` to see what goes wrong there (maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
